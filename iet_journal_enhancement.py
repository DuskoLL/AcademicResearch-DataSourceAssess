#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
IETæœŸåˆŠå¢å¼ºæ¨¡å—
ä¸ºæ»¡è¶³IETæœŸåˆŠå‘è¡¨è¦æ±‚è€Œè®¾è®¡çš„å®éªŒå¢å¼ºå·¥å…·

ä¸»è¦åŠŸèƒ½ï¼š
1. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
2. å®éªŒå¯é‡å¤æ€§éªŒè¯
3. æ–¹æ³•è®ºå®Œæ•´æ€§æ£€æŸ¥
4. ç»“æœå¯é æ€§åˆ†æ
5. ç¬¦åˆIETæ ‡å‡†çš„å®éªŒæŠ¥å‘Šç”Ÿæˆ
"""

import json
import os
import time
import numpy as np
import pandas as pd
from collections import defaultdict
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from scipy import stats
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

@dataclass
class ExperimentalResults:
    """å®éªŒç»“æœæ•°æ®ç»“æ„"""
    total_sources: int
    categories: Dict[str, int]
    grade_distribution: Dict[str, int]
    performance_metrics: Dict[str, float]
    statistical_tests: Dict[str, Any]
    reproducibility_score: float
    methodology_completeness: float

class IETJournalEnhancer:
    """IETæœŸåˆŠæ ‡å‡†å¢å¼ºå™¨"""
    
    def __init__(self, state_dir: str = "state"):
        self.state_dir = state_dir
        self.reports_dir = os.path.join(state_dir, "reports")
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)
        
    def load_experimental_data(self) -> Dict[str, Any]:
        """åŠ è½½å®éªŒæ•°æ®"""
        master_path = os.path.join(self.state_dir, "master_table.json")
        chain_path = os.path.join(self.state_dir, "chain.json")
        validation_path = os.path.join(self.state_dir, "validation_report.json")
        
        data = {}
        
        # åŠ è½½ä¸»è¡¨æ•°æ®
        if os.path.exists(master_path):
            with open(master_path, 'r', encoding='utf-8') as f:
                data['master'] = json.load(f)
        
        # åŠ è½½åŒºå—é“¾æ•°æ®
        if os.path.exists(chain_path):
            with open(chain_path, 'r', encoding='utf-8') as f:
                data['chain'] = json.load(f)
        
        # åŠ è½½éªŒè¯æŠ¥å‘Š
        if os.path.exists(validation_path):
            with open(validation_path, 'r', encoding='utf-8') as f:
                data['validation'] = json.load(f)
        
        return data
    
    def perform_statistical_significance_tests(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•"""
        print("ğŸ”¬ æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•...")
        
        sources = data['master'].get('sources', {})
        if not sources:
            return {}
        
        # æå–ç‰¹å¾æ•°æ®
        features_by_grade = defaultdict(list)
        all_features = []
        all_grades = []
        
        feature_names = ['accuracy', 'availability', 'response_time', 'volatility', 
                        'update_frequency', 'integrity', 'error_rate', 'historical']
        
        for source_id, source_data in sources.items():
            grade = source_data.get('label', 'D')
            features = source_data.get('features', {})
            
            if features:
                feature_vector = [float(features.get(f, 0.0)) for f in feature_names]
                features_by_grade[grade].append(feature_vector)
                all_features.append(feature_vector)
                all_grades.append(grade)
        
        results = {}
        
        # 1. ANOVAæµ‹è¯• - æ£€éªŒä¸åŒç­‰çº§é—´ç‰¹å¾æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
        if len(features_by_grade) > 2:
            for i, feature_name in enumerate(feature_names):
                grade_feature_values = []
                for grade in ['A+', 'A', 'B', 'C', 'D']:
                    if grade in features_by_grade and features_by_grade[grade]:
                        values = [fv[i] for fv in features_by_grade[grade]]
                        grade_feature_values.append(values)
                
                if len(grade_feature_values) > 2:
                    f_stat, p_value = stats.f_oneway(*grade_feature_values)
                    results[f'anova_{feature_name}'] = {
                        'f_statistic': float(f_stat),
                        'p_value': float(p_value),
                        'significant': bool(p_value < 0.05)
                    }
        
        # 2. Kruskal-Wallisæµ‹è¯• - éå‚æ•°æ›¿ä»£æ–¹æ¡ˆ
        if len(features_by_grade) > 2:
            for i, feature_name in enumerate(feature_names):
                grade_feature_values = []
                for grade in ['A+', 'A', 'B', 'C', 'D']:
                    if grade in features_by_grade and features_by_grade[grade]:
                        values = [fv[i] for fv in features_by_grade[grade]]
                        grade_feature_values.append(values)
                
                if len(grade_feature_values) > 2:
                    h_stat, p_value = stats.kruskal(*grade_feature_values)
                    results[f'kruskal_{feature_name}'] = {
                        'h_statistic': float(h_stat),
                        'p_value': float(p_value),
                        'significant': bool(p_value < 0.05)
                    }
        
        # 3. å¡æ–¹ç‹¬ç«‹æ€§æµ‹è¯• - æµ‹è¯•ç­‰çº§åˆ†å¸ƒçš„ç‹¬ç«‹æ€§
        if len(set(all_grades)) > 1:
            grade_counts = pd.Series(all_grades).value_counts()
            chi2_stat, p_value = stats.chisquare(grade_counts.values)
            results['chi_square_grade_distribution'] = {
                'chi2_statistic': float(chi2_stat),
                'p_value': float(p_value),
                'significant': bool(p_value < 0.05)
            }
        
        return results
    
    def validate_clustering_performance(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯èšç±»æ€§èƒ½"""
        print("ğŸ¯ éªŒè¯èšç±»ç®—æ³•æ€§èƒ½...")
        
        sources = data['master'].get('sources', {})
        if not sources:
            return {}
        
        # å‡†å¤‡æ•°æ®
        X = []
        y = []
        feature_names = ['accuracy', 'availability', 'response_time', 'volatility', 
                        'update_frequency', 'integrity', 'error_rate', 'historical']
        
        for source_id, source_data in sources.items():
            grade = source_data.get('label', 'D')
            features = source_data.get('features', {})
            
            if features:
                feature_vector = [float(features.get(f, 0.0)) for f in feature_names]
                X.append(feature_vector)
                y.append(grade)
        
        if len(X) < 10:
            return {'error': 'Insufficient data for clustering validation'}
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(X)
        
        # æ ‡ç­¾ç¼–ç 
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        
        # æ„å»ºKNNåˆ†ç±»å™¨ä½œä¸ºèšç±»è´¨é‡è¯„ä¼°
        knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # äº¤å‰éªŒè¯
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        # å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
        accuracy_scores = cross_val_score(knn, X_scaled, y_encoded, cv=cv, scoring='accuracy')
        f1_scores = cross_val_score(knn, X_scaled, y_encoded, cv=cv, scoring='f1_macro')
        precision_scores = cross_val_score(knn, X_scaled, y_encoded, cv=cv, scoring='precision_macro')
        recall_scores = cross_val_score(knn, X_scaled, y_encoded, cv=cv, scoring='recall_macro')
        
        results = {
            'cross_validation_accuracy': {
                'mean': float(np.mean(accuracy_scores)),
                'std': float(np.std(accuracy_scores)),
                'scores': [float(s) for s in accuracy_scores]
            },
            'cross_validation_f1': {
                'mean': float(np.mean(f1_scores)),
                'std': float(np.std(f1_scores)),
                'scores': [float(s) for s in f1_scores]
            },
            'cross_validation_precision': {
                'mean': float(np.mean(precision_scores)),
                'std': float(np.std(precision_scores)),
                'scores': [float(s) for s in precision_scores]
            },
            'cross_validation_recall': {
                'mean': float(np.mean(recall_scores)),
                'std': float(np.std(recall_scores)),
                'scores': [float(s) for s in recall_scores]
            },
            'sample_size': len(X),
            'feature_count': len(feature_names),
            'class_count': len(le.classes_),
            'class_distribution': {
                grade: int(np.sum(np.array(y) == grade)) for grade in le.classes_
            }
        }
        
        return results
    
    def assess_reproducibility(self, data: Dict[str, Any]) -> float:
        """è¯„ä¼°å®éªŒå¯é‡å¤æ€§"""
        print("ğŸ”„ è¯„ä¼°å®éªŒå¯é‡å¤æ€§...")
        
        score = 0.0
        max_score = 100.0
        
        # 1. æ•°æ®å®Œæ•´æ€§ (30åˆ†)
        sources = data['master'].get('sources', {})
        if sources:
            complete_sources = 0
            for source_data in sources.values():
                if isinstance(source_data, dict) and source_data.get('features'):
                    complete_sources += 1
            
            completeness_ratio = complete_sources / len(sources) if sources else 0
            score += completeness_ratio * 30
        
        # 2. ç®—æ³•å‚æ•°è®°å½• (20åˆ†)
        if 'validation' in data and data['validation'].get('statistics'):
            score += 20
        
        # 3. åŒºå—é“¾æ•°æ®ä¸€è‡´æ€§ (25åˆ†)
        if 'chain' in data:
            blocks = data['chain'].get('blocks', [])
            if blocks:
                # æ£€æŸ¥åŒºå—é“¾å®Œæ•´æ€§
                valid_blocks = 0
                for block in blocks:
                    if isinstance(block, dict) and all(k in block for k in ['index', 'timestamp', 'previous_hash']):
                        valid_blocks += 1
                
                blockchain_integrity = valid_blocks / len(blocks) if blocks else 0
                score += blockchain_integrity * 25
        
        # 4. é…ç½®æ–‡ä»¶å­˜åœ¨æ€§ (15åˆ†)
        config_path = os.path.join(os.path.dirname(self.state_dir), "config.yaml")
        if os.path.exists(config_path):
            score += 15
        
        # 5. æ—¥å¿—è®°å½• (10åˆ†)
        logs_dir = os.path.join(os.path.dirname(self.state_dir), "logs")
        if os.path.exists(logs_dir) and os.listdir(logs_dir):
            score += 10
        
        return min(score, max_score)
    
    def check_methodology_completeness(self, data: Dict[str, Any]) -> float:
        """æ£€æŸ¥æ–¹æ³•è®ºå®Œæ•´æ€§"""
        print("ğŸ“‹ æ£€æŸ¥æ–¹æ³•è®ºå®Œæ•´æ€§...")
        
        score = 0.0
        max_score = 100.0
        
        # 1. å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ (25åˆ†)
        sources = data['master'].get('sources', {})
        if sources:
            sample_source = next(iter(sources.values()))
            features = sample_source.get('features', {})
            expected_features = ['accuracy', 'availability', 'response_time', 'volatility', 
                               'update_frequency', 'integrity', 'error_rate', 'historical']
            
            feature_coverage = len([f for f in expected_features if f in features]) / len(expected_features)
            score += feature_coverage * 25
        
        # 2. èšç±»ç®—æ³•å®ç° (20åˆ†)
        # æ£€æŸ¥æ˜¯å¦æœ‰èšç±»ç›¸å…³çš„ä»£ç å®ç°
        clustering_files = ['clustering.py', 'oracle_chain.py']
        base_dir = os.path.dirname(self.state_dir)
        
        clustering_implemented = any(
            os.path.exists(os.path.join(base_dir, f)) for f in clustering_files
        )
        if clustering_implemented:
            score += 20
        
        # 3. æ•°æ®è´¨é‡éªŒè¯ (20åˆ†)
        if 'validation' in data and data['validation'].get('fixes_applied'):
            score += 20
        
        # 4. åŒºå—é“¾å…±è¯†æœºåˆ¶ (15åˆ†)
        if 'chain' in data:
            blocks = data['chain'].get('blocks', [])
            if blocks and any('proposals' in block for block in blocks):
                score += 15
        
        # 5. ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ– (10åˆ†)
        reports_files = os.listdir(self.reports_dir) if os.path.exists(self.reports_dir) else []
        if any(f.endswith(('.png', '.eps', '.svg')) for f in reports_files):
            score += 5
        if any('report' in f.lower() for f in reports_files):
            score += 5
        
        # 6. APIå¤šæ ·æ€§ (10åˆ†)
        categories = data['master'].get('sources', {})
        if categories:
            unique_categories = set()
            for source_data in categories.values():
                if isinstance(source_data, dict):
                    unique_categories.add(source_data.get('category', 'unknown'))
            
            if len(unique_categories) >= 10:  # 10ä¸ªæˆ–æ›´å¤šä¸åŒç±»åˆ«
                score += 10
        
        return min(score, max_score)
    
    def generate_iet_compliance_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆIETæœŸåˆŠåˆè§„æ€§æŠ¥å‘Š"""
        print("ğŸ“Š ç”ŸæˆIETæœŸåˆŠåˆè§„æ€§æŠ¥å‘Š...")
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        statistical_tests = self.perform_statistical_significance_tests(data)
        clustering_performance = self.validate_clustering_performance(data)
        reproducibility_score = self.assess_reproducibility(data)
        methodology_score = self.check_methodology_completeness(data)
        
        # è®¡ç®—æ€»ä½“åˆ†æ•°
        overall_score = (reproducibility_score + methodology_score) / 2
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            'timestamp': self.timestamp,
            'overall_compliance_score': overall_score,
            'reproducibility_score': reproducibility_score,
            'methodology_completeness': methodology_score,
            'statistical_significance_tests': statistical_tests,
            'clustering_validation': clustering_performance,
            'iet_requirements_check': {
                'experimental_design': {
                    'score': methodology_score,
                    'status': 'PASS' if methodology_score >= 70 else 'NEEDS_IMPROVEMENT',
                    'requirements': [
                        'Multi-dimensional evaluation metrics âœ“',
                        'Clustering algorithm implementation âœ“',
                        'Data quality validation âœ“',
                        'Consensus mechanism âœ“',
                        'Statistical analysis âœ“',
                        'API diversity âœ“'
                    ]
                },
                'reproducibility': {
                    'score': reproducibility_score,
                    'status': 'PASS' if reproducibility_score >= 70 else 'NEEDS_IMPROVEMENT',
                    'requirements': [
                        'Data completeness âœ“',
                        'Algorithm parameters recorded âœ“',
                        'Blockchain data consistency âœ“',
                        'Configuration files âœ“',
                        'Logging system âœ“'
                    ]
                },
                'statistical_rigor': {
                    'score': len([t for t in statistical_tests.values() 
                                if isinstance(t, dict) and t.get('significant', False)]) * 10,
                    'significant_tests': len([t for t in statistical_tests.values() 
                                            if isinstance(t, dict) and t.get('significant', False)]),
                    'total_tests': len(statistical_tests)
                }
            },
            'recommendations': self._generate_recommendations(overall_score, statistical_tests, clustering_performance)
        }
        
        return report
    
    def _generate_recommendations(self, overall_score: float, statistical_tests: Dict, clustering_performance: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if overall_score < 70:
            recommendations.append("æ€»ä½“åˆè§„æ€§åˆ†æ•°è¾ƒä½ï¼Œéœ€è¦æ”¹è¿›å®éªŒè®¾è®¡å’Œæ•°æ®è´¨é‡")
        
        if statistical_tests:
            significant_tests = [t for t in statistical_tests.values() 
                               if isinstance(t, dict) and t.get('significant', False)]
            if len(significant_tests) < len(statistical_tests) * 0.5:
                recommendations.append("å»ºè®®å¢åŠ æ›´å¤šå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„å®éªŒç»“æœ")
        
        if clustering_performance and 'cross_validation_accuracy' in clustering_performance:
            acc_mean = clustering_performance['cross_validation_accuracy']['mean']
            if acc_mean < 0.8:
                recommendations.append("èšç±»ç®—æ³•å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹æˆ–ç®—æ³•å‚æ•°")
        
        if not recommendations:
            recommendations.append("å®éªŒè®¾è®¡ç¬¦åˆIETæœŸåˆŠè¦æ±‚ï¼Œå»ºè®®ç»§ç»­ä¿æŒé«˜è´¨é‡æ ‡å‡†")
        
        return recommendations
    
    def save_compliance_report(self, report: Dict[str, Any]) -> str:
        """ä¿å­˜åˆè§„æ€§æŠ¥å‘Š"""
        report_path = os.path.join(self.reports_dir, "iet_journal_compliance_report.json")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ç”Ÿæˆæ–‡æœ¬ç‰ˆæœ¬
        text_report_path = os.path.join(self.reports_dir, "iet_journal_compliance_report.txt")
        self._generate_text_report(report, text_report_path)
        
        return report_path
    
    def _generate_text_report(self, report: Dict[str, Any], output_path: str):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„åˆè§„æ€§æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("IETæœŸåˆŠåˆè§„æ€§è¯„ä¼°æŠ¥å‘Š\n")
            f.write("Oracleæ•°æ®æºè¯„ä¼°åŒºå—é“¾ç³»ç»Ÿ\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {report['timestamp']}\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("1. æ€»ä½“è¯„ä¼°\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ€»ä½“åˆè§„æ€§åˆ†æ•°: {report['overall_compliance_score']:.1f}/100\n")
            f.write(f"å¯é‡å¤æ€§åˆ†æ•°: {report['reproducibility_score']:.1f}/100\n")
            f.write(f"æ–¹æ³•è®ºå®Œæ•´æ€§: {report['methodology_completeness']:.1f}/100\n\n")
            
            f.write("2. IETæœŸåˆŠè¦æ±‚æ£€æŸ¥\n")
            f.write("-" * 40 + "\n")
            
            req_check = report['iet_requirements_check']
            for category, details in req_check.items():
                if isinstance(details, dict) and 'score' in details:
                    status = details.get('status', 'N/A')
                    f.write(f"{category.title()}: {details['score']:.1f}/100 ({status})\n")
            
            f.write("\n3. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•ç»“æœ\n")
            f.write("-" * 40 + "\n")
            stats_tests = report['statistical_significance_tests']
            if stats_tests:
                significant_count = len([t for t in stats_tests.values() 
                                       if isinstance(t, dict) and t.get('significant', False)])
                f.write(f"æ˜¾è‘—æ€§æµ‹è¯•é€šè¿‡: {significant_count}/{len(stats_tests)}\n")
                
                for test_name, result in stats_tests.items():
                    if isinstance(result, dict):
                        significance = "âœ“" if result.get('significant', False) else "âœ—"
                        f.write(f"  {test_name}: p={result.get('p_value', 0):.4f} {significance}\n")
            else:
                f.write("æœªæ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•\n")
            
            f.write("\n4. èšç±»æ€§èƒ½éªŒè¯\n")
            f.write("-" * 40 + "\n")
            clustering = report['clustering_validation']
            if clustering and 'cross_validation_accuracy' in clustering:
                acc = clustering['cross_validation_accuracy']
                f1 = clustering['cross_validation_f1']
                f.write(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {acc['mean']:.3f} Â± {acc['std']:.3f}\n")
                f.write(f"äº¤å‰éªŒè¯F1åˆ†æ•°: {f1['mean']:.3f} Â± {f1['std']:.3f}\n")
                f.write(f"æ ·æœ¬æ•°é‡: {clustering['sample_size']}\n")
                f.write(f"ç‰¹å¾æ•°é‡: {clustering['feature_count']}\n")
                f.write(f"ç±»åˆ«æ•°é‡: {clustering['class_count']}\n")
            
            f.write("\n5. æ”¹è¿›å»ºè®®\n")
            f.write("-" * 40 + "\n")
            for i, recommendation in enumerate(report['recommendations'], 1):
                f.write(f"{i}. {recommendation}\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨IETæœŸåˆŠæ ‡å‡†å¢å¼ºåˆ†æ...")
    
    enhancer = IETJournalEnhancer()
    
    # åŠ è½½å®éªŒæ•°æ®
    data = enhancer.load_experimental_data()
    
    if not data:
        print("âŒ æ— æ³•åŠ è½½å®éªŒæ•°æ®ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿå·²è¿è¡Œå¹¶ç”Ÿæˆäº†æ•°æ®")
        return False
    
    # ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Š
    compliance_report = enhancer.generate_iet_compliance_report(data)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = enhancer.save_compliance_report(compliance_report)
    
    print(f"âœ… IETæœŸåˆŠåˆè§„æ€§æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"ğŸ“Š æ€»ä½“åˆè§„æ€§åˆ†æ•°: {compliance_report['overall_compliance_score']:.1f}/100")
    
    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
    if compliance_report['overall_compliance_score'] >= 80:
        print("ğŸ‰ å®éªŒè®¾è®¡å®Œå…¨ç¬¦åˆIETæœŸåˆŠå‘è¡¨æ ‡å‡†ï¼")
    elif compliance_report['overall_compliance_score'] >= 70:
        print("âœ… å®éªŒè®¾è®¡åŸºæœ¬ç¬¦åˆIETæœŸåˆŠè¦æ±‚ï¼Œå»ºè®®è¿›è¡Œå°å¹…ä¼˜åŒ–")
    else:
        print("âš ï¸ å®éªŒè®¾è®¡éœ€è¦æ”¹è¿›ä»¥æ»¡è¶³IETæœŸåˆŠæ ‡å‡†")
    
    print("\nğŸ“‹ æ”¹è¿›å»ºè®®:")
    for i, recommendation in enumerate(compliance_report['recommendations'], 1):
        print(f"  {i}. {recommendation}")
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ† IETæœŸåˆŠæ ‡å‡†å¢å¼ºåˆ†æå®Œæˆï¼")
    else:
        print("\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
